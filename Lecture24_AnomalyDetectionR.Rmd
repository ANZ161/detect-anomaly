---
title: "Lecture 23 - Time-Series. Feature Engineering"
output:
  html_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook

### Lecture 24 - Your project. Specialized R packages, AnomalyDetection

Hello everybody. Welcome back to one another bonus lecture. This one will be dedicated to specialized packages in R that are helping us to detect anomalies. We will see how to use package 'AnomalyDetection' on the example on our data.

As usual, we will get a brief review of the functionalities and see how can we apply that in our ShinyApp

#### Package AnomalyDetection

This package can be found on github.com: https://github.com/twitter/AnomalyDetection

Main idea is that we provide our data to the function which will attempt to get the anomalies taking assumption that **Anomaly** is "Something different" from usual! Function is using Seasonal Hybrid `ESD` method. Note: `ESD` stands for *Extreme Studentized Deviate*. It is **computationally intensive** method. Intuitevely, it is using statistical methods. Decomposition of time-series, finding `the robust statistical median`, `Median Absolute Deviation`... not a **Deep Learning**... 

Apparently, as described in this paper [https://arxiv.org/pdf/1704.07706.pdf](https://arxiv.org/pdf/1704.07706.pdf), **twitter** is using this to detect traffic anomalies and provision additional computing capacity. This way, everybody can send their tweet even during extraordinary events. For example, there is some news and everybody has to say something all at the same time!!! 

##### Extracting data

In order to move things forward we will use the data from one process and one machine. Following chunk of code will get the data for us:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)

# ============= READ DATA =================
# Read our big data first ... 9 mln rows...
DF_Data_All <- readRDS("DF_Data_Process.data")
# data about equipment
DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data_All %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)

# ============= END OF READ DATA =================

```

Further on we will extract one process

```{r}
# extract data for one machine and one sub-process
DF_M3_Tub_Res <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #3") %>% 
  select(StartDate, AnalogVal) %>% 
  arrange(StartDate)

dim(DF_M3_Tub_Res) # 332620 rows

```
```{r}
head(DF_M3_Tub_Res)
```


##### Apply function of anomaly detection

Once we have our data frames with one parameter we can directly provide that to the function

```{r}
# applied to our data
res = AnomalyDetectionTs(DF_M3_Tub_Res, max_anoms=0.02, direction='both', plot=TRUE)
# getting out plot
res$plot
```

Things to notice function is quite long to run for a normal CPU. Nevertheless it works. 

Intresting to see  that function runs very nice to detect anomalies! 

##### Getting to know the outputs

Let's learn what is in the returned object `res`. Our goal will be to retrieve only the observations marked with anomalies!

```{r}
# saves just dataframe with anomalies
anomalies <- res$anoms
# notice class of time variable
class(anomalies$timestamp)

```
```{r}
# convert to POSICct
anomalies$timestamp <- as.POSIXct(anomalies$timestamp)
class(anomalies$timestamp)
```
```{r}
head(anomalies)
```


```{r}
# visualize only anomalies
plot(anomalies)
```

Interesting that quite a lot of anomalies is there detected...

#### Your turn... 

Try to filter out another parameter for another machine...

```{r}
# extract data for one machine and one sub-process
DF_M2_Tub_Res <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #2") %>% 
  select(StartDate, AnalogVal) %>% 
  arrange(StartDate)

dim(DF_M2_Tub_Res) # 97493 rows

```
```{r}
head(DF_M2_Tub_Res)
```

Now repeat for this machine...

```{r}
# applied to our data
res2 = AnomalyDetectionTs(DF_M2_Tub_Res, max_anoms=0.02, direction='both', plot=TRUE)
# getting out plot
res2$plot
```




It would be interesting now to output this anomaly `label` to our original dataframe then to use it in our ShinyApp or for other Machine Learning Experiments...

##### Change time series data class



Now we can join data back

```{r}
DF_M3_Tub_Res_Anom <- DF_M3_Tub_Res %>% 
  # joining by time frames
  left_join(anomalies, by = c("StartDate" = "timestamp"))
plot(DF_M3_Tub_Res_Anom$anoms)

```



```{r}
# extract data for one machine and one sub-process
DF_M1_Tub_Res <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #1") %>% 
  select(StartDate, AnalogVal) %>% 
  arrange(StartDate)

dim(DF_M1_Tub_Res) # 344084 rows

```

```{r}
# applied to our data
res3 <- DF_M1_Tub_Res %>% 
  head(50000) %>% 
  AnomalyDetectionTs(max_anoms=0.02, direction='both', plot=TRUE)
# getting out plot
res3$plot
```

I also found out that it's sensitive to the data quality... for example, this portion of data will not be digested...

```{r}
# applied to our data
res3.1 <- DF_M1_Tub_Res %>% 
  #head(50999) %>%
  AnomalyDetectionTs(max_anoms=0.02, direction='both', plot=TRUE, longterm = TRUE, only_last = 'day')
# getting out plot
res3.1$plot
```

```{r}
DF_M1_Tub_Res %>% 
  head(60000) %>% plot()
```




```{r}
# applied to our data
res5 <- DF_M1_Tub_Res %>% 
  tail(150000) %>% 
  AnomalyDetectionTs(max_anoms=0.05, direction='both', plot=TRUE)
# getting out plot
res5$plot
```
Hmm package is not at so much robust... :()


It seems that we are discovering really something interesting!!!

Before we go into implementing that in our ShinyApp let's play with parameters a bit

##### direction 'pos' or 'neg'

This parameter will allow us to detect only anomalies that are more 'positive' from the mean. This might be quite relevant for example in cases when you know the nature of the data you are looking into...

```{r}
# applied to our data
res4 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='pos', plot=TRUE)
# getting out plot
res4$plot
```

##### only_last

If you are looking to only report anomalies in the last day or hour this parameter can be set up

```{r}
# applied to our data
res5 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='pos', only_last = 'day', plot=TRUE)
# getting out plot
res5$plot
```

##### piecewise_median_period_weeks

Function learns to detect anomaly taking into account some period of data. By default it is 2 weeks. For that function will not work if we will supply data that is less than that

```{r}
# get less rows to run function
DF_M3_Tub_Res_5000 <- DF_M3_Tub_Res %>% head(5000)
```

Error will be given

```{r}
# applied to our data
res5.1 = AnomalyDetectionTs(DF_M3_Tub_Res_5000, max_anoms=0.02, direction='pos', 
                          y_log = TRUE, plot=TRUE)
# getting out plot
res5.1$plot
```

##### longterm_period

For long time series we can increase detection eficacy

```{r}
# applied to our data
res5.3 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='pos',longterm = T, 
                            plot=TRUE)
# getting out plot
res5.3$plot
```


##### y_log

We can benefit from log scaling, in our case we don't really have large positive anomalies...

```{r}
# applied to our data
res6 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='pos', 
                          y_log = TRUE, plot=TRUE)
# getting out plot
res6$plot
```

#### Implementation in Shiny...

In order to implement that in Shiny we will need to take into account several facts:

* longer time to run function
* must use larger dataset in order to allow function to run properly

For that the idea will be to use the larger dataset and to limit number of rows for each machine/ process to be maximum 50000

We will only keep new functionality for the App
