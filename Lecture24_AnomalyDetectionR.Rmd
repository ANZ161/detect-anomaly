---
title: "Lecture 23 - Time-Series. Feature Engineering"
output:
  html_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook

### Lecture 24 - Your project. Specialized R packages, AnomalyDetection

Hello everybody. Welcome back to one another bonus lecture. This one will be dedicated to specialized packages in R that are helping us to detect anomalies. We will see how to use package 'AnomalyDetection' on the example on our data.

As usual, we will get a brief review of the functionalities and see how can we apply that in our ShinyApp

#### Package AnomalyDetection

This package can be found on github.com: https://github.com/twitter/AnomalyDetection

Main idea is that we provide our data to the function which will attempt to get the anomalies taking assumption that **Anomaly** is "Something different" from usual! Function is using Seasonal Hybrid ESD method.

Intuitevely, it is using statistical method where we have to provide parameters explicitly. 

##### Extracting data

In order to move things forward we will use the data from one process and one machine. Following chunk of code will get the data for us:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)

# ============= READ DATA =================
# Read our big data first ... 9 mln rows...
DF_Data_All <- readRDS("DF_Data_Process.data")
# data about equipment
DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data_All %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)

# ============= END OF READ DATA =================

```

Further on we will extract two process for two different machines

```{r}
# extract data for one machine and one sub-process
DF_M3_Cut_Ph <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #3") %>% 
  select(StartDate, AnalogVal)

dim(DF_M3_Cut_Ph) # 82883 rows

# extract data for one machine and one sub-process
DF_M3_Tub_Res <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #3") %>% 
  select(StartDate, AnalogVal) %>% 
  arrange(StartDate)

dim(DF_M3_Tub_Res) # 332620 rows

```


##### Apply function of anomaly detection

Once we have our data frames with one parameter we can directly provide that to the function

```{r}
# applied to our data
res = AnomalyDetectionTs(DF_M3_Cut_Ph, max_anoms=0.02, direction='both', plot=TRUE)
# getting out plot
res$plot
```

Things to notice function is quite long to run for a normal CPU. Nevertheless it works. A bit weird to state that there are 2% of anomalies before running the function, inst't it...

And for another machine where we have 332000 rows this function was simply not giving result after 10 minutes:

```{r}
# applied to our data
res2 = AnomalyDetectionTs(DF_M3_Tub_Res, max_anoms=0.02, direction='both', plot=TRUE)
# getting out plot
res2$plot
```

```{r}
# get less rows to run function
DF_M3_Tub_Res_Last <- DF_M3_Tub_Res %>% tail(50000)
```

```{r}
# applied to our data
res2 = AnomalyDetectionTs(DF_M3_Tub_Res_Last, max_anoms=0.02, direction='both', plot=TRUE)
# getting out plot
res2$plot
```

Intresting to see  that function runs very nice to detect anomalies! 

How was that on the beginning of the period?

```{r}
# get less rows to run function
DF_M3_Tub_Res_Rec <- DF_M3_Tub_Res %>% head(50000)
```

```{r}
# applied to our data
res3 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='both', plot=TRUE)
# getting out plot
res3$plot
```

It seems that we are discovering really something interesting!!!

Before we go into implementing that in our ShinyApp let's play with parameters a bit

##### direction 'pos' or 'neg'

This parameter will allow us to detect only anomalies that are more 'positive' from the mean. This might be quite relevant for example in cases when you know the nature of the data you are looking into...

```{r}
# applied to our data
res4 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='pos', plot=TRUE)
# getting out plot
res4$plot
```

##### only_last

If you are looking to only report anomalies in the last day or hour this parameter can be set up

```{r}
# applied to our data
res5 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='pos', only_last = 'day', plot=TRUE)
# getting out plot
res5$plot
```

##### y_log

We can benefit from log scaling, in our case we don't really have large positive anomalies...

```{r}
# applied to our data
res6 = AnomalyDetectionTs(DF_M3_Tub_Res_Rec, max_anoms=0.02, direction='pos', 
                          y_log = TRUE, plot=TRUE)
# getting out plot
res6$plot
```

#### Implementation in Shiny...

##### Used Libraries...

We will stat by including the required libraries. We will use `xts` to help us to manage time-series data...

```{r, message=FALSE, warning=FALSE}
#
library(tidyverse)
library(xts)
```

Then we will perform data import and showing one example of feature engineering in `R`

```{r, message=FALSE, warning=FALSE}
# ============= READ DATA =================
# Read our small data second ... 
DF_Data_Recent <- readRDS("DF_Data_Process_Recent.data") 

DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data_Recent %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)

# ============= END OF READ DATA =================
```

##### Our strategy

 1. For each specific machine solve the problem: 
   - generate specific dataset
   - convert to xts object
   - find periodicity, esplore data
   - aggregate by hour and create new features
   - merge into one object
   - return back to the dataframe
   - join back initial features of the object
 2. Create function, apply this to every single machine, process...
 3. Deploy for ShinyApp
 4. Verify on the dataset & compare...

##### JOIN, Visualize DATA

```{r}
# We will take one process for Machine #1 and visualize it...
DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = Name)) + geom_point()
```

##### Implement for one specific process of one machine

```{r}
# get only data for specific sub-process
DF_M1_Cut_Ph <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  select(StartDate, AnalogVal)
```

Now we are ready to transform our data to `xts object`

```{r}
# create xts object (matrix and index)
xts_M1_Cut_Ph <- as.xts(DF_M1_Cut_Ph[, -1], order.by = as.POSIXct(DF_M1_Cut_Ph$StartDate))
```

Some important quality checks is to check periocity of our data (e.g. seconds, minutes...)

```{r}
# getting to know the perioficity of the data and the time span
periodicity(xts_M1_Cut_Ph)
# getting to know number of hours, seconds, etc
nseconds(xts_M1_Cut_Ph)
nhours(xts_M1_Cut_Ph)
# also plot data for quality control
plot.xts(xts_M1_Cut_Ph)
```

At this stage we can make apply some functions to the populations of the data during each hour. In this example it's simple `mean`, `max`, `min`, `sd` or Standard Deviation...
```{r}
# convert periodicity from seconds to hours and apply some functions to create new features
AnalogVal_mean <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), mean)
AnalogVal_max <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), max)
AnalogVal_Min <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), min)
AnalogVal_sd <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), sd)
# review obtained object and merging several columns (just for better visualization...)
head(merge(AnalogVal_mean, AnalogVal_max, AnalogVal_Min, AnalogVal_sd))
```

We can of course merge and store these new columns in one `xts` object. We should not worry because `xts` will make sure our data rows are matching index [time]

```{r}
# merge these features together
xts_CUT_Ph <- merge(AnalogVal_mean, AnalogVal_max, AnalogVal_Min, AnalogVal_sd)

# view dataset we have now
head(xts_CUT_Ph)

```

```{r}
# always good to see a plot
# Use plot.zoo to plot these two columns in a single panel
plot.zoo(xts_CUT_Ph[,c("AnalogVal_mean", "AnalogVal_sd")], plot.type = "multiple")
```


##### Returning back to DataFrame

It's now possible to return to our original DataFrame
```{r}
# now we can return to the dataframe
cd <- coredata(xts_CUT_Ph) %>% as.data.frame(row.names = F) 
cd$StartDate <- index(xts_CUT_Ph)

# view dataset we have now
head(cd)
```





##### Turn this into function

```{r}
feature_eng_ts <- function(x, funcToApply = c("mean", "min", "max", "sd")){
  #function will return new dataframe with new features
  #x = dataframe with columns to perform feature engineering, it must contain data from one specific category
  #tscolname = column name containing time series data
  #avcolname - column name contining data to perform manipulations
  #funcToApply - vector containing functions to apply data transformations
  # convert to xts object
  DF_M1_xts <- as.xts(x[ ,-1], order.by = as.POSIXct(x$StartDate))
  # aggregate by hour and create new features
  # convert periodicity from seconds to hours and apply some functions to create new features
  for(i in 1: length(funcToApply)){
    # apply functions and merge them  
    res <- period.apply(DF_M1_xts, endpoints(DF_M1_xts, "hours"), funcToApply[i])
    names(res) <- paste("AnalogVal_", funcToApply[i], sep = "")
    # merge new features together
    if(i == 1){
      DF_M1_newfeatures <- res
      } else {
        DF_M1_newfeatures <- merge(DF_M1_newfeatures, res)
        }
  }
  # return to the dataframe
  DF_M1_NF <- coredata(DF_M1_newfeatures) %>% as.data.frame(row.names = F)
  DF_M1_NF$StartDate <- index(DF_M1_newfeatures)
  # return result
  return(DF_M1_NF)
}

```


##### Example of usage

Let's try to see how to use our function... 

```{r}
# we can apply transformation to each specific machine and each specific sub-process
DF <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  select(StartDate, AnalogVal)

# keep just categories of the data: machine and event text
DF_Cat <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% select(EventText, Name) %>% unique()

# get new dataset with new features
DF_FE <- feature_eng_ts(DF)
# join original categories
DF_FE$EventText <- DF_Cat[1,1]
DF_FE$Name <- DF_Cat[1,2]

# review obtained dataset
head(DF_FE)

# plot some variables
ggplot(DF_FE, aes(x = StartDate, y = AnalogVal_mean, col = Name)) + geom_point()

```

At the first look, we have probably obtained much better separation between normality / anormality... we will see how it will work in our `ShinyApp`...


#### Your turn... [practice]

Let'me suggest you to do some practical activity in order to 'live' the moment of feature engineering... Use R-Notebook and copy/paste code adapting for your specific needs to achieve the goal

* Try to create similar data transformation using function for another 'Event' for example 'Tubing process resistance'. 
* Try to change the function `feature_eng_ts` in a way that `mad()` function is added [Median Absolute Deviation]