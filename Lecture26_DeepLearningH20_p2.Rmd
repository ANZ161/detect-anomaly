---
title: "Lecture 26 - Into Deep Learning P2"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook


### Lecture 26 - Your project. Deep Learning with H2O. P.2 Implement on our example

Deep Learning... This lecture is dedicated to the implementation of Deep Learning models into R and in particular **ShinyApp**

#### Work overview (from previous lecture)

* re-arranging data as matrix
* fitting deep learning models
* testing the models
* saving models
* implementation in our ShinyApp...

The idea will be to use one particular feature e.g. **Tubing Process, Impedance** to play with

#### Re-arranging data as a matrix

Let's get our time - series data as a dataframe first...

```{r, message=FALSE, warning=FALSE}
library(tidyverse)

# ============= READ DATA =================
# Read our small data ... 
DF_Data_Recent <- readRDS("DF_Data_Process_Recent.data") 

DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data_Recent %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)
```

Then I will plot my data for all 4 machines as just to remember how it looks like...

```{r}
# creating human readable data and visualize them
DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = Name)) + geom_point()+facet_grid(~Name)
```

Looking on the chart above I can see that machine #1 seems to be the best. I will take that one as a reference to build my Deep Learning Model. I will be extracting this dataset with a **filter()** function

```{r}
# extracting only one machine
DF_M1 <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #3") %>% 
  select(StartDate, AnalogVal) %>% 
  head(50)
```

#### Transposing the data

Now we need to **transpose** our data from long to wide structure!

```{r}
# transposing our dataframe
DF_t <- as.data.frame(t(as.matrix(DF_M1)))
head(DF_t)
```

Now, we can actually 'forget' the StartDate values and simply parse the values into matrix of dimension say 200 columns and 20 rows... Of course one need to recover some basic R skills for that :) if not use stackoverflow... (How to turn a vector into a matrix in R?)[https://stackoverflow.com/questions/14614946/how-to-turn-a-vector-into-a-matrix-in-r]

```{r}
DF_M1 <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #1") %>% 
  select(AnalogVal) %>% 
  t() %>%  # this brings us a matrix
  as.vector() %>% # let's make it a vector
  head(3000) %>% # only making fixed amount of rows
  matrix(nrow = 20, ncol = 150) # transforming that into matrix size 25x200
```

Wonderful, let's try to see! our new object as an image!!!

#### Explore the matrix as a surface!

Let's use `plotly` 3D graph to explore what we have got!

```{r}
library(plotly)
plot_ly(z = DF_M1, type = "surface")

```

This should be good enough to fit our deep learning model, but before we will do so I will 'prepare' my test datasets

A. For machine 2:

```{r}
DF_M2 <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #2") %>% 
  select(AnalogVal) %>% 
  t() %>%  # this brings us a matrix
  as.vector() %>% # let's make it a vector
  head(1500) %>% # only making fixed amount of rows
  matrix(nrow = 10, ncol = 150) # transforming that into matrix size 25x200
```

B. For machine 3:

```{r}
DF_M3 <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #3") %>% 
  select(AnalogVal) %>% 
  t() %>%  # this brings us a matrix
  as.vector() %>% # let's make it a vector
  head(3000) %>% # only making fixed amount of rows
  matrix(nrow = 20, ncol = 150) # transforming that into matrix size 25x200
```

And we can also visualize that to confront:

For Machine 2

```{r}
plot_ly(z = DF_M2, type = "surface")
```

For Machine 3

```{r}
plot_ly(z = DF_M3, type = "surface")
```



#### Fitting the Deep Learning Model

Launch the machine again...

```{r, eval=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# to load the library
library(h2o)

# to initialize the 'machine'
localH2O = h2o.init()
```

#### Load datasets to H2O

Then we will download the datasets into h2o. Remember H2O is just operated from R but it's a computer besides!


```{r, eval=TRUE, include=TRUE}
# Import train data into the H2O cluster
train_M1 <- as.h2o(x = DF_M1, destination_frame = "train_M1")

# Also import our test datasets for Machines 2 and 3...
test_M2  <- as.h2o(x = DF_M2, destination_frame = "test_M2")
test_M3  <- as.h2o(x = DF_M3, destination_frame = "test_M3")

```


#### Building Deep Learning model with autoencoder

Now, once we know how our data looks like we can start to do our Anomaly Model

```{r, eval=TRUE, include=TRUE}

# Train deep autoencoder learning model on "normal" 
# training data, y ignored 
anomaly_model <- h2o.deeplearning(
 x = names(train_ecg), 
 training_frame = train_ecg, 
 activation = "Tanh", 
 autoencoder = TRUE, 
 hidden = c(50,20,50), 
 sparse = TRUE,
 l1 = 1e-4, 
 epochs = 100)
```

#### Calculate MSE from the train dataset

Let's use this model on our training dataset...

```{r, eval=TRUE, include=TRUE}
# computer error of the model
mod_error <- h2o.anomaly(anomaly_model, train_ecg)
```

get it as a plot and see that the value is very low

```{r, eval=TRUE, include=TRUE}
# visually see it
h2o.anomaly(anomaly_model, train_ecg) %>% 
  as.data.frame() %>% plot.ts(ylim = c(0, 2))
```

#### Detect Anomaly using MSE value

Once our model is made, we can use it to detect anomalies in our **test** dataset


```{r, eval=TRUE, include=TRUE}
# Compute reconstruction error with the Anomaly 
# detection app (MSE between output and input layers)
recon_error <- h2o.anomaly(anomaly_model, test_ecg)

# Pull reconstruction error data into R and 
# plot to find outliers (last 3 heartbeats)
df_recon_error <- as.data.frame(recon_error)
tail(df_recon_error, 9)
```

We can plot this as well
```{r, eval=TRUE, include=TRUE}
plot.ts(df_recon_error)
```

What it is telling us is that based on the new data we have anomaly in elements 21, 22, 23

#### Use Anomaly model to reconstruct Test Dataset

Now we can obtain predictions, or physical values using our model. We should provide the model and test dataset

```{r, eval=TRUE, include=TRUE}
# Note: Testing = Reconstructing the test dataset
test_recon <- h2o.predict(anomaly_model, test_ecg) 
```

In order to make things visible. Once again I ask excel to help... Here I simply write teh dataframe to csv and create graph in excel

```{r, eval=FALSE, include=TRUE}
# write to csv to use it in excel
test_recon %>% as.data.frame() %>% 
write.csv("test_predicted.csv")
```


#### Visualize as 3D

Or we can visualize in 3D directly in R
```{r, eval=F, include=TRUE}
# making a matrix dataframe
recon_matrix <- test_recon %>% as.data.frame() %>% as.matrix.data.frame()

# make 3D plot
plot_ly(z = recon_matrix, type = "surface")
```



#### Saving Deep Learning Model for the future use

To use our model in our ShinyApp we will save it...

```{r, eval=FALSE, include=TRUE}
h2o.saveModel(anomaly_model, "C:/Users/fxtrams/Downloads/tmp/anomaly_model.bin")
h2o.download_pojo(anomaly_model, "C:/Users/fxtrams/Downloads/tmp", get_jar = TRUE)
```

And let's not forget to switch off our cluster!
```{r, eval=TRUE, include=TRUE}
h2o.shutdown(prompt= FALSE)

```

#### Conclusion

In this example the Anomaly Detection model was able to output the anomaly in rows 21-23. 

It learned on the pattern of many vectors and was able to distinguish the anomaly coming on new dataset

Practical use of this model can be to us function **h2o.anomaly**. In case the MSE value will be high - the anomaly is detected!

#### Next step

our next step will be to repeat the procedure but on our machine data. 

* re-arranging data as matrix
* fitting deep learning models
* testing the models
* saving models
* implementation in our ShinyApp...

#### used references

example from: (https://dzone.com/articles/anomaly-detection-with-deep-learning-in-r-with-h2o)[https://dzone.com/articles/anomaly-detection-with-deep-learning-in-r-with-h2o]

More reading: (https://dzone.com/articles/the-basics-of-deep-learning-how-to-apply-it-to-pre?fromrel=true)[https://dzone.com/articles/the-basics-of-deep-learning-how-to-apply-it-to-pre?fromrel=true]

And: (https://shiring.github.io/machine_learning/2017/05/01/fraud)[https://shiring.github.io/machine_learning/2017/05/01/fraud]

paper: (https://arxiv.org/abs/1701.01887)[https://arxiv.org/abs/1701.01887]
In this lecture we would explore the 'technology' on the sample and try to do this in 10 min lecture!



