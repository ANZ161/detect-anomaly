---
title: "Lecture 26 - Into Deep Learning P2"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook


### Lecture 26 - Your project. Deep Learning with H2O. P.2 Implement on our example

Deep Learning... This lecture is dedicated to the implementation of Deep Learning models into R and in particular **ShinyApp**

#### Work overview (from previous lecture)

* re-arranging data as matrix
* fitting deep learning models
* testing the models
* saving models
* implementation in our ShinyApp...

The idea will be to use one particular feature e.g. **Tubing Process, Impedance** to play with

#### Re-arranging data as a matrix

Let's get our time - series data as a dataframe first...

```{r, message=FALSE, warning=FALSE}
library(tidyverse)

# ============= READ DATA =================
# Read our small data ... 
DF_Data_Recent <- readRDS("DF_Data_Process_Recent.data") 

DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data_Recent %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)
```

Then I will plot my data for all 4 machines as just to remember how it looks like...

```{r}
# creating human readable data and visualize them
DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = Name)) + geom_point()+facet_grid(~Name)
```

Looking on the chart above I can see that machine #1 seems to be the best. I will take that one as a reference to build my Deep Learning Model. I will be extracting this dataset with a **filter()** function

```{r}
# extracting only one machine
DF_M1 <- DF_TEMP %>% 
  filter(EventText == "Tubing Process, resistance Ohm") %>% 
  filter(Name == "Machine #3") %>% 
  select(StartDate, AnalogVal) %>% 
  head(50)
```

#### Transposing the data

Now we need to **transpose** our data from long to wide structure!

```{r}
# transposing our dataframe
DF_t <- as.data.frame(t(as.matrix(DF_M1)))
head(DF_t)
```

Now, we can actually 'forget' the StartDate values and simply parse the values into matrix of dimension say 200 columns and 20 rows...



#### Explore the 'thing' on example

We will now run the demo from H2O. Our goal will be to teach system what is `normal` by using ECG dataset. After that we will use another dataset that will contain `anomaly` and use our Deep Learning model to detect that

#### Start Virtual Machine h2o

First thing we will launch the machine again...

```{r, eval=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# to load the library
library(h2o)

# to initialize the 'machine'
h2o.init()
```

#### Load datasets from H2O

Then we will download the datasets from h2o.

```{r, eval=TRUE, include=TRUE}
# Import ECG train and test data into the H2O cluster
train_ecg <- h2o.importFile(
 path = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/anomaly/ecg_discord_train.csv", 
 header = FALSE, 
 sep = ",")
test_ecg <- h2o.importFile(
 path = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/anomaly/ecg_discord_test.csv", 
 header = FALSE, 
 sep = ",")
```

#### Understanding dataset

Personally I like to know what is in the data and how it look like!!! I will just use this link and put it into the browser. This will download the files with raw data. If I open this dataset it would not tell me much! I will plot this data in excel...



Or, I can also pull the data from h2o into R and make some 3D visualizations...

```{r, eval=TRUE, include=TRUE}
library(tidyverse)
# data frame matrix for training dataset
matrix_train <- train_ecg %>% as.data.frame() %>% as.matrix.data.frame()
# data frame matrix for test dataset
matrix_test <- test_ecg %>% as.data.frame() %>% as.matrix.data.frame()
```

from there we can see that the difference between both are in the three new rows 21-23

```{r, eval=FALSE, include=TRUE}
# using library plotly to plot 3D surface
library(plotly)
plot_ly(z = matrix_train, type = "surface")

```


```{r, eval=FALSE, include=TRUE}
# using library plotly to plot 3D surface
plot_ly(z = matrix_test, type = "surface")

```


#### Building Deep Learning model with autoencoder

Now, once we know how our data looks like we can start to do our Anomaly Model

```{r, eval=TRUE, include=TRUE}

# Train deep autoencoder learning model on "normal" 
# training data, y ignored 
anomaly_model <- h2o.deeplearning(
 x = names(train_ecg), 
 training_frame = train_ecg, 
 activation = "Tanh", 
 autoencoder = TRUE, 
 hidden = c(50,20,50), 
 sparse = TRUE,
 l1 = 1e-4, 
 epochs = 100)
```

#### Calculate MSE from the train dataset

Let's use this model on our training dataset...

```{r, eval=TRUE, include=TRUE}
# computer error of the model
mod_error <- h2o.anomaly(anomaly_model, train_ecg)
```

get it as a plot and see that the value is very low

```{r, eval=TRUE, include=TRUE}
# visually see it
h2o.anomaly(anomaly_model, train_ecg) %>% 
  as.data.frame() %>% plot.ts(ylim = c(0, 2))
```

#### Detect Anomaly using MSE value

Once our model is made, we can use it to detect anomalies in our **test** dataset


```{r, eval=TRUE, include=TRUE}
# Compute reconstruction error with the Anomaly 
# detection app (MSE between output and input layers)
recon_error <- h2o.anomaly(anomaly_model, test_ecg)

# Pull reconstruction error data into R and 
# plot to find outliers (last 3 heartbeats)
df_recon_error <- as.data.frame(recon_error)
tail(df_recon_error, 9)
```

We can plot this as well
```{r, eval=TRUE, include=TRUE}
plot.ts(df_recon_error)
```

What it is telling us is that based on the new data we have anomaly in elements 21, 22, 23

#### Use Anomaly model to reconstruct Test Dataset

Now we can obtain predictions, or physical values using our model. We should provide the model and test dataset

```{r, eval=TRUE, include=TRUE}
# Note: Testing = Reconstructing the test dataset
test_recon <- h2o.predict(anomaly_model, test_ecg) 
```

In order to make things visible. Once again I ask excel to help... Here I simply write teh dataframe to csv and create graph in excel

```{r, eval=FALSE, include=TRUE}
# write to csv to use it in excel
test_recon %>% as.data.frame() %>% 
write.csv("test_predicted.csv")
```


#### Visualize as 3D

Or we can visualize in 3D directly in R
```{r, eval=F, include=TRUE}
# making a matrix dataframe
recon_matrix <- test_recon %>% as.data.frame() %>% as.matrix.data.frame()

# make 3D plot
plot_ly(z = recon_matrix, type = "surface")
```



#### Saving Deep Learning Model for the future use

To use our model in our ShinyApp we will save it...

```{r, eval=FALSE, include=TRUE}
h2o.saveModel(anomaly_model, "C:/Users/fxtrams/Downloads/tmp/anomaly_model.bin")
h2o.download_pojo(anomaly_model, "C:/Users/fxtrams/Downloads/tmp", get_jar = TRUE)
```

And let's not forget to switch off our cluster!
```{r, eval=TRUE, include=TRUE}
h2o.shutdown(prompt= FALSE)

```

#### Conclusion

In this example the Anomaly Detection model was able to output the anomaly in rows 21-23. 

It learned on the pattern of many vectors and was able to distinguish the anomaly coming on new dataset

Practical use of this model can be to us function **h2o.anomaly**. In case the MSE value will be high - the anomaly is detected!

#### Next step

our next step will be to repeat the procedure but on our machine data. 

* re-arranging data as matrix
* fitting deep learning models
* testing the models
* saving models
* implementation in our ShinyApp...

#### used references

example from: (https://dzone.com/articles/anomaly-detection-with-deep-learning-in-r-with-h2o)[https://dzone.com/articles/anomaly-detection-with-deep-learning-in-r-with-h2o]

More reading: (https://dzone.com/articles/the-basics-of-deep-learning-how-to-apply-it-to-pre?fromrel=true)[https://dzone.com/articles/the-basics-of-deep-learning-how-to-apply-it-to-pre?fromrel=true]

And: (https://shiring.github.io/machine_learning/2017/05/01/fraud)[https://shiring.github.io/machine_learning/2017/05/01/fraud]

paper: (https://arxiv.org/abs/1701.01887)[https://arxiv.org/abs/1701.01887]
In this lecture we would explore the 'technology' on the sample and try to do this in 10 min lecture!



