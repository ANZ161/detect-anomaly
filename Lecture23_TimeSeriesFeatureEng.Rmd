---
title: "Lecture 23 - Time-Series. Feature Engineering"
output:
  html_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook

### Lecture 23 - Your project. Feature Engineering on Time Series

#### What is Feature Engineering

`Feature Engineering` is a process to create new data from original data:

* Better explaining, summarizing feature
* We can use Domain Knowledge
* Best to keep minimum...

#### How to perform Feature Engineering on Time-Series data?

The best way is to use specialized libraries such as `xts` that will transform our `Data Table` into `xts object`. We can perform statistical, matematical transformations on data and return back to our `Data Frame`

#### Implementation in R

##### Used Libraries...

We will stat by including the required libraries. We will use `xts` to help us to manage time-series data...

```{r}
#
library(tidyverse)
library(xts)
```

Then we will perform data import and showing one example of feature engineering in `R`

```{r, message=FALSE, warning=FALSE}
# ============= READ DATA =================
# Read our small data second ... 
DF_Data_Recent <- readRDS("DF_Data_Process_Recent.data") 

DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data_Recent %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)

# ============= END OF READ DATA =================
```

##### Our strategy

 1. For each specific machine solve the problem: 
   - generate specific dataset
   - convert to xts object
   - find periodicity, esplore data
   - aggregate by hour and create new features
   - merge into one object
   - return back to the dataframe
   - join back initial features of the object
 2. Create function, apply this to every single machine, process...
 3. Deploy for ShinyApp
 4. Verify on the dataset & compare...

##### JOIN, Visualize DATA

```{r}
# We will take one process for Machine #1 and visualize it...
DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = Name)) + geom_point()
```

##### Implement for one specific process of one machine

```{r}
# get only data for specific sub-process
DF_M1_Cut_Ph <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  select(StartDate, AnalogVal)
```

Now we are ready to transform our data to `xts object`

```{r}
# create xts object (matrix and index)
xts_M1_Cut_Ph <- as.xts(DF_M1_Cut_Ph[, -1], order.by = as.POSIXct(DF_M1_Cut_Ph$StartDate))
```

Some important quality checks is to check periocity of our data (e.g. seconds, minutes...)

```{r}
# getting to know the perioficity of the data and the time span
periodicity(xts_M1_Cut_Ph)
# getting to know number of hours, seconds, etc
nseconds(xts_M1_Cut_Ph)
nhours(xts_M1_Cut_Ph)
```

At this stage we can make apply some functions to the populations of the data during each hour. In this example it's simple `mean`, `max`, `min`, `sd` or Standard Deviation...
```{r}
# convert periodicity from seconds to hours and apply some functions to create new features
AnalogVal_mean <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), mean)
AnalogVal_max <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), max)
AnalogVal_Min <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), min)
AnalogVal_sd <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), sd)
# review obtained object and merging several columns (just for better visualization...)
head(merge(AnalogVal_mean, AnalogVal_max, AnalogVal__Min, AnalogVal_sd))
```

We can of course merge and store these new columns in one `xts` object. We should not worry because `xts` will make sure our data rows are matching index [time]

```{r}
# merge these features together
xts_CUT_Ph <- merge(AnalogVal_mean, AnalogVal_max, AnalogVal_Min, AnalogVal_sd)

# view dataset we have now
head(xts_CUT_Ph)
```

##### Returning back to DataFrame

It's now possible to return to our original DataFrame
```{r}
# now we can return to the dataframe
cd <- coredata(xts_CUT_Ph) %>% as.data.frame(row.names = F) 
cd$StartDate <- index(xts_CUT_Ph)

# view dataset we have now
head(cd)
```





##### Turn this into function

```{r}
feature_eng_ts <- function(x, funcToApply = c("mean", "min", "mad", "sd")){
  #function will return new dataframe with new features
  #x = dataframe with columns to perform feature engineering, it must contain data from one specific category
  #tscolname = column name containing time series data
  #avcolname - column name contining data to perform manipulations
  #funcToApply - vector containing functions to apply data transformations
  # convert to xts object
  DF_M1_xts <- as.xts(x[-1], order.by = as.POSIXct(x$StartDate))
  # aggregate by hour and create new features
  # convert periodicity from seconds to hours and apply some functions to create new features
  for(i in 1: length(funcToApply)){
    # apply functions and merge them  
    res <- period.apply(DF_M1_xts, endpoints(DF_M1_xts, "hours"), funcToApply[i])
    names(res) <- paste("AnalogVal_", funcToApply[i], sep = "")
    # merge new features together
    if(i == 1){
      DF_M1_newfeatures <- res
      } else {
        DF_M1_newfeatures <- merge(DF_M1_newfeatures, res)
        }
  }
  # return to the dataframe
  DF_M1_NF <- coredata(DF_M1_newfeatures) %>% as.data.frame(row.names = F)
  DF_M1_NF$StartDate <- index(DF_M1_newfeatures)
  # return result
  return(DF_M1_NF)
}

```


##### Example of usage

Let's try to see how to use our function... 

```{r}
# we can apply transformation to each specific machine and each specific sub-process
DF <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  select(StartDate, AnalogVal)

# keep just categories of the data: machine and event text
DF_Cat <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% select(EventText, Name) %>% unique()

# get new dataset with new features
DF_FE <- feature_eng_ts(DF)
# join original categories
DF_FE$EventText <- DF_Cat[1,1]
DF_FE$Name <- DF_Cat[1,2]

# review obtained dataset
head(DF_FE)

# plot some variables
ggplot(DF_FE, aes(x = StartDate, y = AnalogVal_mad, col = Name)) + geom_point()

```

At the first look, we have probably obtained much better separation between normality / anormality... we will see how it will work in our `ShinyApp`...


#### Your turn... [practice]

Let'me suggest you to do some practical activity in order to 'live' the moment of feature engineering...

* Try to create similar data transformation using function 
* Try to change the function `feature_eng_ts` in a way that `mad()` function is added [Median Absolute Deviation]