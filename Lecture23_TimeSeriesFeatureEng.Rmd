---
title: "Lecture 23 - Time-Series. Feature Engineering"
output:
  html_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook

### Lecture 23 - Your project. Feature Engineering on Time Series

#### What is Feature Engineering

`Feature Engineering` is a process to create new data from original data:

* Better explaining, summarizing feature
* We can use Domain Knowledge
* Best to keep minimum...

#### How to perform Feature Engineering on Time-Series data?

The best way is to use specialized libraries such as `xts` that will transform our `Data Table` into `xts object`. We can perform statistical, matematical transformations on data and return back to our `Data Frame`

#### Implementation in R

##### Used Libraries...

We will stat by including the required libraries. We will use `xts` to help us to manage time-series data...

```{r}
#
library(tidyverse)
library(xts)
```

Then we will perform data import and showing one example of feature engineering in `R`

```{r, message=FALSE, warning=FALSE}
# ============= READ DATA =================
# Read our small data second ... 
DF_Data_Recent <- readRDS("DF_Data_Process_Recent.data") 

DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data_Recent %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)

# ============= END OF READ DATA =================
```

##### Our strategy

 1. For each specific machine solve the problem: 
   - convert to xts object
   - find periodicity, esplore data
   - aggregate by hour and create new features
   - merge into one object
   - return back to the dataframe
   - join back initial features of the object
 2. Create function, apply this to every single machine, process...
 3. Deploy for ShinyApp
 4. Verify on the dataset & compare...

##### JOIN, Visualize DATA

```{r}
# We will take one process for Machine #1 and visualize it...
DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = Name)) + geom_point()
```

##### Implement for one specific process of one machine

```{r}
# get only data for specific sub-process
DF_M1_Cut_Ph <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  select(StartDate, AnalogVal)
```

Now we are ready to transform our data to `xts object`

```{r}
# create xts object (matrix and index)
xts_M1_Cut_Ph <- as.xts(DF_M1_Cut_Ph[, -1], order.by = as.POSIXct(DF_M1_Cut_Ph$StartDate))
```

Some important quality checks is to check periocity of our data (e.g. seconds, minutes...)

```{r}
# getting to know the perioficity of the data and the time span
periodicity(xts_M1_Cut_Ph)
# getting to know number of hours, seconds, etc
nseconds(xts_M1_Cut_Ph)
nhours(xts_M1_Cut_Ph)
```

At this stage we can make apply some functions to the populations of the data during each hour. In this example it's simple `mean`, `max`, `min`, `sd` or Standard Deviation...
```{r}
# convert periodicity from seconds to hours and apply some functions to create new features
AnalogVal_mean <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), mean)
AnalogVal_max <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), max)
AnalogVal_Min <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), min)
AnalogVal_sd <- period.apply(xts_M1_Cut_Ph, endpoints(xts_M1_Cut_Ph, "hours"), sd)
# review obtained object and merging several columns (just for better visualization...)
head(merge(AnalogVal_mean, AnalogVal_max, AnalogVal__Min, AnalogVal_sd))
```

We can of course merge and store these new columns in one `xts` object. We should not worry because `xts` will make sure our data rows are matching index [time]

```{r}
# merge these features together
xts_CUT_Ph <- merge(AnalogVal_mean, AnalogVal_max, AnalogVal_Min, AnalogVal_sd)

# view dataset we have now
head(xts_CUT_Ph)
```

##### Returning back to DataFrame

It's now possible to return to our original DataFrame
```{r}
# now we can return to the dataframe
cd <- coredata(xts_CUT_Ph) %>% as.data.frame(row.names = F) 
cd$StartDate <- index(xts_CUT_Ph)

# view dataset we have now
head(cd)
```





##### Turn this into function

```{r}
feature_eng_ts <- function(x, funcToApply = c("mean", "min", "max", "sd")){
  #function will return new dataframe with new features
  #x = dataframe with columns to perform feature engineering, it must contain data from one specific category
  #tscolname = column name containing time series data
  #avcolname - column name contining data to perform manipulations
  #funcToApply - vector containing functions to apply data transformations
  # convert to xts object
  DF_M1_xts <- as.xts(x[-1], order.by = as.POSIXct(x$StartDate))
  # aggregate by hour and create new features
  # convert periodicity from seconds to hours and apply some functions to create new features
  for(i in 1: length(funcToApply)){
    # apply functions and merge them  
    res <- period.apply(DF_M1_xts, endpoints(DF_M1_xts, "hours"), funcToApply[i])
    names(res) <- paste("AnalogVal_", funcToApply[i], sep = "")
    # merge new features together
    if(i == 1){
      DF_M1_newfeatures <- res
      } else {
        DF_M1_newfeatures <- merge(DF_M1_newfeatures, res)
        }
  }
  # return to the dataframe
  DF_M1_NF <- coredata(DF_M1_newfeatures) %>% as.data.frame(row.names = F)
  DF_M1_NF$StartDate <- index(DF_M1_newfeatures)
  # return result
  return(DF_M1_NF)
}

```


# example of usage
# we can apply transformation to each specific machine and each specific sub-process
DF <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% 
  select(StartDate, AnalogVal)

# keep just categories of the data: machine and event text
DF_Cat <- DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  filter(Name == "Machine #1") %>% select(EventText, Name) %>% unique()

# get new dataset with new features
DF_FE <- feature_eng_ts(DF)
# join original categories
DF_FE$EventText <- DF_Cat[1,1]
DF_FE$Name <- DF_Cat[1,2]

# review obtained dataset
head(DF_FE)

# plot some variables
ggplot(DF_FE, aes(x = StartDate, y = AnalogVal_sd))+geom_point()

```

We can use this data to better understand the process and come up with some sort of strategy on how we can detect anomaly.

Apart of that we have also a smaller data set with more 'recent' data. This data is from several days of operation.

```{r}
# Read our small data second ... 
DF_Data_Process_Recent <- readRDS("DF_Data_Process_Recent.data")
# Dimension of dataset
dim(DF_Data_Process_Recent)
```

#### Join and visualize the data

We can make some better overview of data starting from 'recent' dataset. We join this dataset to more human readable format...

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# ============= JOIN DATA =================
# data frame containing information from multiple sensors
DF_Data <- readRDS("DF_Data_Process_Recent.data")
# data frame containing equipment information
DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)
```

We can visualize this dataset, putting all the points together...

```{r}
# Visualize the data...
DF_TEMP %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = EventText)) + geom_point()+facet_grid(~Name)
```

Almost impossible to gather any insights. It's just too much data in the graph...

#### Join and visualize the data - Your turn

1. Create separate visualizations for all 9 processes (use sample code below to start with).

```{r}
# Visualize the data...
DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = EventText)) + geom_point()+facet_grid(~Name)
```

Hint: you can copy/paste proposed code, write a `for` loop, etc

For your convenience, it's worth to use `for` loop using example as below

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
# We can for loop those as well
library(magrittr)
# save our Event names into the vector 'Events'
Events <- DF_TEMP %>% select(EventText) %>% unique() %$% EventText
# make a for loop...
for(i in 1:length(Events)){
plots <- '...' %>% 
  filter(EventText == Events[i]) %>% 
  ggplot('...') + geom_point()+facet_grid(~Name) 
# and we will print plots
print(plots)
}
```


2. Can you already observe some differences between the same parameter in different machines? Which ones?

Hint: Pay attention on parameters level, how are those parameters distributed, etc

#### Same for larger dataset

1. Import and join bigger dataset

Hint: Use file `DF_Data_Process.data` to import bigger dataset

2. Create a generalized plot for that dataset

Hint: Use the for loop construct once again... data is quite big so running this code may take a while... also feel free to use `geom_smooth()` instead of `geom_point()`

3. Can you observe some tendencies in the parameters in different machines? Which ones?

Hint: Don't bother too much looking into that data. Answer can be even negative! Sometime human can not sence any tendency - it's normal!!!