---
title: "Lecture 18 - Homework"
output:
  html_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook

### Lecture 18 - Your project. Introducing the new dataset

#### Nature of our data

Data we are dealing with in our project coming from `arbutrary manufacturing process`. We are in simplified converting process where we have:

* Four Machines producing Metal Cans
* Each Machine convert Raw Material into Product
* Each Converting Process has 3 Sub-Process:

1. Edging
2. Tubing
3. Cutting

Each Sub-Process have 3 parameters we can monitor, 2 of them are indicating 'process' parameters. The process is not yet studied very precisely and can vary without creating stoppages of the system.

Our goal will be to identify anomaly in this process without knowing at what level is the anomaly apriori.

At the moment we have a data from the past from 4 machines that are exactly the same. Process parameters should behave similarly for all machines.

#### Business need

As mentioned in the lecture, process parameters may vary quite a lot and it's not possible to set up very precise system that would give alerts and proper indication. For example, we can not set up the 'warning' or 'alarm' levels to this data programmatically. Perhaps this warning level will work for one particular process but will completely fail just on another batch of raw material... At the same time it's important to keep these parameters undercontrol and perhaps even to come up with a strategy to detect deviations before the real problem would occur.

Perhaps the idea can be to 'learn' from bigger dataset and use the new dataset to identify the anomaly? For example, factory can do a weekly review of data and have an overview of the situation.

#### Review of available data

We can read data stored in the file **DF_Data_Process.data**. This is `R` object serialized and stored as a file. This dataset contains more than 9 mln rows. 

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# read data into R environment
# ============= READ DATA =================
# Read our big data first ... 9 mln rows...
DF_Data_Process_All <- readRDS("DF_Data_Process.data")
# Dimension of dataset
dim(DF_Data_Process_All)
```

We can use this data to better understand the process and come up with some sort of strategy on how we can detect anomaly.

Apart of that we have also a smaller data set with more 'recent' data. This data is from several days of operation.

```{r}
# Read our small data second ... 
DF_Data_Process_Recent <- readRDS("DF_Data_Process_Recent.data")
# Dimension of dataset
dim(DF_Data_Process_Recent)
```

#### Join and visualize the data

We can make some better overview of data starting from 'recent' dataset. We join this dataset to more human readable format...

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# ============= JOIN DATA =================
# data frame containing information from multiple sensors
DF_Data <- readRDS("DF_Data_Process_Recent.data")
# data frame containing equipment information
DF_Equipm <- read_csv("DF_EquipmData.csv")
# data frame containing Event Names
DF_EvCode <- read_csv("DF_EvCodeDataProject.csv")

# Data manipulation and saving to the DF_TEMP
DF_TEMP <- DF_Data %>% 
  # join to decode equipment serial number
  inner_join(DF_Equipm, by = "IDEquipment") %>% 
  # join to decode Event Code meaning
  inner_join(DF_EvCode, by = "EventCode") %>% 
  # select only column needed
  select(StartDate, Name, AnalogVal, EventText)
```

We can visualize this dataset, putting all the points together...

```{r}
# Visualize the data...
DF_TEMP %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = EventText)) + geom_point()+facet_grid(~Name)
```

Almost impossible to gather any insights. It's just too much data in the graph...

#### Join and visualize the data - Your turn

1. Create separate visualizations for all 9 processes (use sample code below to start with).

```{r}
# Visualize the data...
DF_TEMP %>% 
  filter(EventText == "Cutting Process, phase angle") %>% 
  ggplot(aes(x = StartDate, y = AnalogVal, col = EventText)) + geom_point()+facet_grid(~Name)
```

Hint: you can copy/paste proposed code, write a `for` loop, etc

For your convenience, it's worth to use `for` loop using example as below

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
# We can for loop those as well
library(magrittr)
# save our Event names into the vector 'Events'
Events <- DF_TEMP %>% select(EventText) %>% unique() %$% EventText
# make a for loop...
for(i in 1:length(Events)){
plots <- '...' %>% 
  filter(EventText == Events[i]) %>% 
  ggplot('...') + geom_point()+facet_grid(~Name) 
# and we will print plots
print(plots)
}
```


2. Can you already observe some differences between the same parameter in different machines? Which ones?

Hint: Pay attention on parameters level, how are those parameters distributed, etc

#### Same for larger dataset

1. Import and join bigger dataset

Hint: Use file `DF_Data_Process.data` to import bigger dataset

2. Create a generalized plot for that dataset

Hint: Use the for loop construct once again... data is quite big so running this code may take a while... also feel free to use `geom_smooth()` instead of `geom_point()`

3. Can you observe some tendencies in the parameters in different machines? Which ones?

Hint: Don't bother too much looking into that data. Answer can be even negative! Sometime human can not sence any tendency - it's normal!!!